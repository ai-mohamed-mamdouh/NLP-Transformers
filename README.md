# NLP-Transformers

This repository contains practical implementations of Transformer-based NLP models such as **BERT**, **GPT**, and other modern architectures.

> âš ï¸ This is not a production project repository.
> It is a personal study reference that includes applied code based on the concepts I am learning.

---

## ðŸ“š Purpose

The main goal of this repository is to:

* Reinforce theoretical knowledge through hands-on coding
* Understand Transformer architecture in depth
* Practice tokenization and fine-tuning
* Experiment with different NLP tasks
* Keep organized notes for revision

This repository represents my practical learning journey in modern NLP.

---

## ðŸ“‚ Repository Structure

```
NLP-Transformers/
â”‚
â”œâ”€â”€ bert/
â”‚   â”œâ”€â”€ Semantic Search
â”‚   â”œâ”€â”€ text-classification/
â”‚   â”œâ”€â”€ ner/
â”‚   â””â”€â”€ question-answering/
â”‚
â”œâ”€â”€ gpt/
â”‚   â”œâ”€â”€ text-generation/
â”‚   â”œâ”€â”€ prompting/
â”‚   â””â”€â”€ fine-tuning/
â”‚
â””â”€â”€ notes/
```

Each folder contains:

* Clean and commented code
* Applied examples
* Practical experiments
* Concept-based implementation

---

## ðŸš€ Topics Covered

* Transformer Architecture
* Attention & Self-Attention
* Encoder vs Decoder Models
* Tokenization (WordPiece / BPE)
* Fine-Tuning Pretrained Models
* Text Classification
* Named Entity Recognition (NER)
* Question Answering
* Text Generation
* Prompt Engineering

---

## ðŸ§  Learning Approach

This repository focuses on understanding how models work internally â€” not just using high-level APIs.

The goal is to:

* Build intuition about Transformer mechanics
* Apply each studied concept directly in code
* Create a structured reference for future revision

---

ðŸ“Œ This repository grows as I continue learning and exploring advanced NLP models.

---
