{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ **Project Overview**\n",
        "\n",
        "ðŸ§  **Project Title**\n",
        "\n",
        "**Question Answering System using BERT (SQuAD Dataset)**\n",
        "\n",
        "---\n",
        "\n",
        "ðŸŽ¯ **Project Goal**\n",
        "\n",
        "The goal of this project is to build a **Question Answering (QA) system** using a pre-trained BERT model.\n",
        "\n",
        "The model learns how to:\n",
        "\n",
        "* Read a paragraph (context)\n",
        "* Understand a question\n",
        "* Find the correct answer inside the paragraph\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ“‚ **Dataset**\n",
        "\n",
        "We use the **SQuAD dataset**.\n",
        "\n",
        "Each example contains:\n",
        "\n",
        "* `context` â†’ A paragraph of text\n",
        "* `question` â†’ A question about the paragraph\n",
        "* `answers` â†’ The correct answer text and its position\n",
        "\n",
        "---\n",
        "\n",
        "âš™ï¸ **Project Steps**\n",
        "\n",
        " - 1ï¸âƒ£ Load Dataset\n",
        "\n",
        "   - We load a small part of SQuAD for training.\n",
        "\n",
        " - 2ï¸âƒ£ Tokenization\n",
        "\n",
        "   - We use BERT tokenizer to convert text into numbers (input_ids).\n",
        "   - We also handle long texts using:\n",
        "\n",
        "       * Truncation\n",
        "       * Stride\n",
        "       * Offset mapping\n",
        "\n",
        " - 3ï¸âƒ£ Preprocessing\n",
        "\n",
        "   - We calculate:\n",
        "\n",
        "    * `start_positions`\n",
        "    * `end_positions`\n",
        "\n",
        "      These represent where the correct answer starts and ends inside the context.\n",
        "\n",
        " - 4ï¸âƒ£ Model\n",
        "\n",
        "   - We use:\n",
        "    `bert-base-uncased`\n",
        "\n",
        "   - Specifically:\n",
        "    `AutoModelForQuestionAnswering`\n",
        "\n",
        "   - This model predicts:\n",
        "\n",
        "     * Start token of answer\n",
        "     * End token of answer\n",
        "\n",
        " - 5ï¸âƒ£ Training\n",
        "\n",
        "    We train the model using HuggingFace `Trainer`.\n",
        "\n",
        " - 6ï¸âƒ£ Evaluation\n",
        "\n",
        "   - We evaluate the model using the SQuAD metric:\n",
        "\n",
        "     * Exact Match (EM)\n",
        "     * F1 Score\n",
        "\n",
        "- 7ï¸âƒ£ Inference\n",
        "\n",
        "  - We use a pipeline to test the model on new text.\n",
        "   - The model returns:\n",
        "\n",
        "     * The predicted answer\n",
        "     * Confidence score\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ **Final Result**\n",
        "\n",
        "At the end of this project:\n",
        "\n",
        "* We have a fine-tuned BERT QA model\n",
        "* The model can answer questions from text\n",
        "* The model can be saved and used in a backend system\n",
        "\n",
        "---\n",
        "ðŸ“Œ **About the Author**\n",
        "\n",
        " - **Name:** **Mohamed Mamdouh**\n",
        " - Student at the Faculty of **Artificial Intelligence**\n",
        "\n",
        " - [**LinkedIn**](https://www.linkedin.com/in/ai-mohamed-mamdouh-74043b331/)\n",
        " - [**GitHub**](https://github.com/ai-mohamed-mamdouh)\n",
        " - [**Kaggle**](https://www.kaggle.com/mohamed00mamdouh)\n"
      ],
      "metadata": {
        "id": "Fk2M4K9K6yjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Project: Question Answering (QA) with BERT on SQuAD\n",
        "# =========================================================\n",
        "# Goal: Train a BERT model to answer questions from a context text.\n",
        "# Input: (context, question, answer_start, answer_text)\n",
        "# Output: Model predicts start_position and end_position of the answer in the context."
      ],
      "metadata": {
        "id": "oq-WKDmn0d4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ukKMaQIc5HyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Load dataset + check transformers version**"
      ],
      "metadata": {
        "id": "_Lb4SKm21a-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# Goal: Load a small part of SQuAD dataset for training.\n",
        "# Why: SQuAD is a famous QA dataset (question + context + answer).\n",
        "dataset = load_dataset(\"squad\", split=\"train[:1000]\")\n",
        "\n",
        "# Goal: Print one example to understand dataset columns.\n",
        "for col in ['id', 'title', 'context', 'question', 'answers']:\n",
        "  print(dataset[0][col])\n",
        "  print('--------------------------------------------------')"
      ],
      "metadata": {
        "id": "44CWixAi0hNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset structure:\")\n",
        "print(dataset)\n",
        "\n",
        "print(\"\\nColumns:\")\n",
        "print(dataset.column_names)\n",
        "\n",
        "print(\"\\nFirst example:\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "28W12wE-4dMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load tokenizer**"
      ],
      "metadata": {
        "id": "fjiO_EKX1hZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "# Goal: Use BERT tokenizer to convert text -> token IDs.\n",
        "# Why: Model needs numbers (input_ids), not raw text.\n",
        "# Note: use_fast=True gives offset mapping (important for QA)."
      ],
      "metadata": {
        "id": "s30-8BK90mym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTokenizer loaded:\")\n",
        "print(tokenizer)\n",
        "\n",
        "print(\"\\nTokenizer vocab size:\")\n",
        "print(tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "LhH04tCN4g3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Preprocess function (tokenize + create labels)**"
      ],
      "metadata": {
        "id": "wW7CMKXu1qcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Goal: Clean questions (remove extra spaces).\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Goal: Tokenize (question, context) together.\n",
        "    # Why: QA model needs both question and context as input.\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,                # Goal: limit max tokens\n",
        "        truncation=\"only_second\",      # Goal: cut only the context if too long\n",
        "        stride=128,                    # Goal: overlap pieces when context is long\n",
        "        return_overflowing_tokens=True,# Goal: create many chunks for long context\n",
        "        return_offsets_mapping=True,   # Goal: keep (start_char, end_char) for each token\n",
        "        padding=\"max_length\",          # Goal: make same length for batch\n",
        "    )\n",
        "\n",
        "    # Goal: Get offset mapping then remove it from inputs.\n",
        "    # Why: We use it to find answer start/end, then we don't need it in final dataset.\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    # Goal: Map each chunk back to original example index.\n",
        "    # Why: One example can produce many chunks because of stride.\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    # Goal: For each chunk, find the answer token start/end positions.\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]        # which original example\n",
        "        answer = answers[sample_idx]      # answers for that example\n",
        "\n",
        "        # Goal: If there is no answer (rare), set 0,0\n",
        "        if len(answer[\"answer_start\"]) == 0:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "            continue\n",
        "\n",
        "        # Goal: Get answer start and end in character level.\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        # Goal: Know which tokens are question (0) and which are context (1)\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Step A: Find where context starts in tokens\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "\n",
        "        # Step B: Find where context ends in tokens\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # Goal: Check if the answer is inside this chunk.\n",
        "        # Why: Some chunks do not contain the answer because of truncation.\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Goal: Find token index for answer start\n",
        "            curr = context_start\n",
        "            while curr <= context_end and offset[curr][0] <= start_char:\n",
        "                curr += 1\n",
        "            start_positions.append(curr - 1)\n",
        "\n",
        "            # Goal: Find token index for answer end\n",
        "            curr = context_end\n",
        "            while curr >= context_start and offset[curr][1] >= end_char:\n",
        "                curr -= 1\n",
        "            end_positions.append(curr + 1)\n",
        "\n",
        "    # Goal: Add QA labels to inputs\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "\n",
        "    print(\"\\nInside preprocess:\")\n",
        "    print(\"Keys:\", inputs.keys())\n",
        "    print(\"input_ids shape:\", len(inputs[\"input_ids\"]))\n",
        "    print(\"start_positions example:\", start_positions[:5])\n",
        "    print(\"end_positions example:\", end_positions[:5])\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "AbNVzuTH0muL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Apply preprocessing on dataset**"
      ],
      "metadata": {
        "id": "5OfrkBz71xHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# Goal: Create training dataset with fields:\n",
        "# input_ids, attention_mask, token_type_ids (maybe), start_positions, end_positions\n",
        "# Why: Trainer needs these tensors to train QA model."
      ],
      "metadata": {
        "id": "A0DynUSp0msS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTokenized dataset:\")\n",
        "print(tokenized_datasets)\n",
        "\n",
        "print(\"\\nFirst tokenized example:\")\n",
        "print(tokenized_datasets[0])\n",
        "\n",
        "print(\"\\ninput_ids length:\")\n",
        "print(len(tokenized_datasets[0][\"input_ids\"]))"
      ],
      "metadata": {
        "id": "q46ydi3O4ukf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Load QA model + define training args**"
      ],
      "metadata": {
        "id": "WEzK1f-a14Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "# Goal: Load BERT model for Question Answering.\n",
        "# Why: This model outputs start logits and end logits.\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Goal: Set training settings (light training).\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-squad\",          # where to save outputs\n",
        "    eval_strategy=\"no\",                 # no evaluation during training (simple)\n",
        "    learning_rate=2e-5,                 # good learning rate for BERT\n",
        "    per_device_train_batch_size=8,      # batch size\n",
        "    num_train_epochs=1,                 # train 1 epoch for test\n",
        "    weight_decay=0.01,                  # regularization\n",
        ")\n",
        "\n",
        "# Goal: Create Trainer to run training loop.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    # tokenizer=tokenizer,              # optional\n",
        ")\n",
        "\n",
        "print(\"Start training ...\")\n",
        "trainer.train()\n",
        "\n",
        "# Output: trained model in memory (and files in output_dir if saving enabled)."
      ],
      "metadata": {
        "id": "7wBiIolP0mqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel loaded:\")\n",
        "print(model)\n",
        "print(\"\\nNumber of parameters:\")\n",
        "print(model.num_parameters())"
      ],
      "metadata": {
        "id": "HUSMU_4m4xRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Example metric usage (NOT real evaluation)**"
      ],
      "metadata": {
        "id": "mCA69zpI18y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# Goal: Load SQuAD metric (Exact Match and F1 for QA).\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "# WARNING:\n",
        "# This part uses fake example (Paris). It is ONLY to show how metric works.\n",
        "predictions = [{'prediction_text': 'Paris', 'id': '1'}]\n",
        "references = [{'answers': {'answer_start': [40], 'text': ['Paris']}, 'id': '1'}]\n",
        "\n",
        "results = metric.compute(predictions=predictions, references=references)\n",
        "print(f\"Results: {results}\")"
      ],
      "metadata": {
        "id": "wshp8gw903Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./our_model\")\n",
        "tokenizer.save_pretrained(\"./our_model\")"
      ],
      "metadata": {
        "id": "dqmxa29Y2JkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Inference with pipeline**"
      ],
      "metadata": {
        "id": "8T0K2Ju12ArA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# IMPORTANT:\n",
        "# You must save model + tokenizer, then load them.\n",
        "\n",
        "# Goal: Build QA pipeline for easy inference.\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"our model\",       # TODO: replace with \"./our_model\" or model object\n",
        "    tokenizer=\"our tokenizer\" # TODO: replace with \"./our_model\" or tokenizer object\n",
        ")\n",
        "\n",
        "#__________________________________________________________________________________\n",
        "# Goal: Test with a long context\n",
        "long_context = \"\"\"\n",
        "The Great Pyramid of Giza is the largest Egyptian pyramid and served as the tomb of pharaoh Khufu.\n",
        "It was built in the early 26th century BC and took around 27 years to compute.\n",
        "It is the oldest of the Seven Wonders of the Ancient World.\n",
        "The pyramid's height was originally 146.6 meters, making it the tallest man-made structure in the world for over 3,800 years.\n",
        "\"\"\"\n",
        "\n",
        "question = \"How long did it take to build the pyramid\"\n",
        "\n",
        "#__________________________________________________________________________________\n",
        "\n",
        "# NOTE:\n",
        "# pipeline(\"question-answering\") usually supports:\n",
        "# question=..., context=...\n",
        "# Some pipeline versions do NOT support max_seq_len/doc_stride/top_k here.\n",
        "# If you get error, remove these params.\n",
        "\n",
        "# Goal: Run inference and get best answers.\n",
        "results = qa_pipeline(\n",
        "    question=question,\n",
        "    context=long_context,\n",
        "    max_seq_len=512,   # may not work in some versions\n",
        "    doc_stride=128,    # may not work in some versions\n",
        "    top_k=3            # may not work in some versions\n",
        ")\n",
        "\n",
        "# Goal: Print answers\n",
        "print(f\"--- Question: {question} ---\")\n",
        "for i, res in enumerate(results):\n",
        "    print(f\"Rank {i+1}:\")\n",
        "    print(f\"   - Answer: {res['answer']}\")\n",
        "    print(f\"   - Score: {round(res['score'], 4)}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "y9Zwi-pq03Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra Notes**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**To use in backend:**\n",
        "- Save model + tokenizer in same folder:\n",
        "  - `trainer.save_model(\"./our_model\")`\n",
        "  - `tokenizer.save_pretrained(\"./our_model\")`\n",
        "\n",
        "- Then load pipeline:\n",
        "   - `qa_pipeline = pipeline(\"question-answering\", model=\"./our_model\", tokenizer=\"./our_model\")`"
      ],
      "metadata": {
        "id": "gvMachqg3LYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "âœ… **Project Summary**\n",
        "\n",
        "In this project, we built a **Question Answering model using BERT and SQuAD**.\n",
        "\n",
        "The model learns to read a paragraph and answer questions by predicting the start and end of the correct answer.\n",
        "\n",
        "We prepared the data, fine-tuned BERT, and tested the model using real examples.\n",
        "\n",
        "The final model can be saved and used in real applications like chatbots and AI systems ðŸš€\n"
      ],
      "metadata": {
        "id": "UVYAzkTt8klZ"
      }
    }
  ]
}