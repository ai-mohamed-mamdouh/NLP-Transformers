{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Search**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Np62f5UyUeSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "M2X1xO0oLKGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query = 'How can I reduce overfitting in a deep learning model?'"
      ],
      "metadata": {
        "id": "V69XCPnIU4Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "\"Applying dropout layers during training helps prevent overfitting in neural networks.\",\n",
        "\n",
        "\n",
        "\"The Eiffel Tower is one of the most visited monuments in the world.\",\n",
        "\n",
        "\n",
        "\"Using regularization techniques such as L2 weight decay can reduce overfitting in deep learning models.\",\n",
        "\n",
        "\n",
        "\"Basketball is played by two teams of five players each.\",\n",
        "\n",
        "\n",
        "\"Artificial intelligence systems rely on large datasets to make predictions.\",\n",
        "\n",
        "\n",
        "\"Mount Everest is the highest mountain above sea level.\",\n",
        "\n",
        "\n",
        "\"Data augmentation is an effective way to improve generalization and reduce overfitting.\",\n",
        "\n",
        "\n",
        "\"The Amazon rainforest is home to millions of species.\",\n",
        "\n",
        "\n",
        "\"Neural networks are composed of layers of interconnected neurons.\",\n",
        "\n",
        "\"Cooking rice requires boiling water and letting it simmer.\",\n",
        "\n",
        "\"Deep learning models require significant computational resources for training.\",\n",
        "\n",
        "\"Soccer is the most popular sport worldwide.\",\n",
        "\n",
        "\"Gradient descent is used to minimize loss functions during training.\",\n",
        "\n",
        "\"Photosynthesis allows plants to convert sunlight into energy.\",\n",
        "\n",
        "\"Hyperparameter tuning can significantly impact model performance.\",\n",
        "\n",
        "\"The Pacific Ocean is the largest ocean on Earth.\",\n",
        "\n",
        "\"Increasing batch size may speed up training but affects convergence.\",\n",
        "\n",
        "\"Ancient Rome was one of the greatest civilizations in history.\",\n",
        "\n",
        "\"Machine learning models can suffer from bias if data is unbalanced.\",\n",
        "\n",
        "\"Shakespeare wrote many famous plays and sonnets.\"\n",
        "]"
      ],
      "metadata": {
        "id": "1siwzQX1UGID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "DvEoOLv4Ld20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emb_text_st(text, st_model):\n",
        "    if isinstance(text, str):\n",
        "        text = [text]\n",
        "\n",
        "    emb = st_model.encode(\n",
        "        text,\n",
        "        convert_to_tensor=True,\n",
        "        normalize_embeddings=True\n",
        "    )  # (N, dim)\n",
        "    return emb\n",
        "\n",
        "def Semantic_Search(query_emb, doc_emb, docs, top_k=3):\n",
        "    scores = torch.matmul(query_emb, doc_emb.T).squeeze(0)  # (N,)\n",
        "    values, indices = torch.topk(scores, k=top_k)\n",
        "\n",
        "    print(f\"Top {top_k} results:\\n\")\n",
        "    for rank, (idx, score) in enumerate(zip(indices, values), start=1):\n",
        "        idx = int(idx)\n",
        "        print(f\"{rank}) score={float(score):.4f}\")\n",
        "        print(docs[idx])\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "3IPzqCpReI1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_emb = emb_text_st(Query, st_model)     # (1, dim)\n",
        "doc_emb   = emb_text_st(docs, st_model)      # (20, dim)\n",
        "\n",
        "Semantic_Search(query_emb, doc_emb, docs, top_k=3)"
      ],
      "metadata": {
        "id": "n-_dPu3uqS7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}