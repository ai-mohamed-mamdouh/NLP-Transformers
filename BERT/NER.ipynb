{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ**Project Overview**\n",
        "\n",
        "This project builds a **Named Entity Recognition (NER)** model using **BERT**.\n",
        "\n",
        " ðŸŽ¯ **Goal**\n",
        "\n",
        "The goal is to train a model that can find and classify entities in text.\n",
        "For example:\n",
        "\n",
        "* Person name\n",
        "* Organization\n",
        "* Skills\n",
        "* Location\n",
        "* `Or any custom labels in the dataset`\n",
        "\n",
        "---\n",
        "\n",
        " ðŸ§ **What the code does**\n",
        "\n",
        "1. **Load Dataset**\n",
        "\n",
        "   * Load a prepared NER dataset from disk.\n",
        "   * Dataset contains tokens (words) and their labels.\n",
        "\n",
        "2. **Prepare Labels**\n",
        "\n",
        "   * Convert label names to numbers.\n",
        "   * Create mapping between label and id.\n",
        "\n",
        "3. **Tokenization**\n",
        "\n",
        "   * Use BERT tokenizer.\n",
        "   * Split words into subwords.\n",
        "   * Align labels with new tokens.\n",
        "\n",
        "4. **Build Model**\n",
        "\n",
        "   * Load `bert-base-uncased`.\n",
        "   * Add token classification head.\n",
        "   * Set number of labels.\n",
        "\n",
        "5. **Prepare Training**\n",
        "\n",
        "   * Use DataCollator for correct padding.\n",
        "   * Set training parameters (learning rate, batch size, epochs).\n",
        "\n",
        "6. **Evaluation**\n",
        "\n",
        "   * Use `seqeval` to calculate:\n",
        "\n",
        "     * Precision\n",
        "     * Recall\n",
        "     * F1 Score\n",
        "     * Accuracy\n",
        "\n",
        "7. **Training**\n",
        "\n",
        "   * Train the model on training data.\n",
        "   * Evaluate on validation data.\n",
        "   * Fine-tune BERT for NER task.\n",
        "\n",
        "---\n",
        "\n",
        " ðŸ **Final Result**\n",
        "\n",
        "After training:\n",
        "\n",
        "* We get a fine-tuned BERT model.\n",
        "* The model can predict entity labels for each word in a sentence.\n",
        "* The model can be saved and used in backend (API or application).\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ“Œ **About the Author**\n",
        "\n",
        " - **Name:** **Mohamed Mamdouh**\n",
        " - Student at the Faculty of **Artificial Intelligence**\n",
        "\n",
        " - [**LinkedIn**](https://www.linkedin.com/in/ai-mohamed-mamdouh-74043b331/)\n",
        " - [**GitHub**](https://github.com/ai-mohamed-mamdouh)\n",
        " - [**Kaggle**](https://www.kaggle.com/mohamed00mamdouh)"
      ],
      "metadata": {
        "id": "VmjbzyWKyWQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "d1X2FicH1o97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports & Load DataSet**"
      ],
      "metadata": {
        "id": "9m4bKbB1xd1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Goal: We import tools for:\n",
        "# - loading dataset (datasets)\n",
        "# - tokenizer + model + training (transformers)\n",
        "# - math (numpy)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Load dataset from disk\n",
        "# =========================\n",
        "from datasets import load_from_disk\n",
        "data = load_from_disk(\"YOUR DATA PATH\")\n",
        "print(data)\n",
        "\n",
        "# Goal: Load the NER dataset saved before.\n",
        "# Why: We want to train a model on this dataset.\n",
        "# Output: A DatasetDict like: {\"train\", \"validation\", \"test\"} (maybe)"
      ],
      "metadata": {
        "id": "1NtL2DaTsdbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build label dictionaries**"
      ],
      "metadata": {
        "id": "K6Tbmmugv8nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Goal: Collect all label IDs used in the dataset.\n",
        "# Example: labels can be 0, 1, 2... or like \"B-SKILL\", \"I-SKILL\", \"O\" (depends on dataset).\n",
        "all_labels = sorted(list({tag for sent in data['train'][\"ner_tags\"] for tag in sent}))\n",
        "\n",
        "# Goal: Create mapping label -> id\n",
        "label2id = {l: i for i, l in enumerate(all_labels)}\n",
        "\n",
        "# Goal: Create mapping id -> label\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "# Why we need this:\n",
        "# - The model works with numbers (ids).\n",
        "# - For metrics and display we want label names too."
      ],
      "metadata": {
        "id": "-3lg9aGCuRL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert ner_tags to labels (ids)**"
      ],
      "metadata": {
        "id": "hY3pvstHwD2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(example):\n",
        "    # Goal: Add a new key \"labels\" to each example.\n",
        "    # Why: Trainer expects the label column to be named \"labels\".\n",
        "    example[\"labels\"] = [label2id[t] for t in example[\"ner_tags\"]]\n",
        "    return example\n",
        "\n",
        "data = data.map(encode_labels)\n",
        "\n",
        "# Output now:\n",
        "# each example has:\n",
        "# - tokens (list of words)\n",
        "# - ner_tags (original labels)\n",
        "# - labels (numeric labels)"
      ],
      "metadata": {
        "id": "fTiqaaUJuy5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize and align labels**"
      ],
      "metadata": {
        "id": "rBTECVzxwK6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Goal: Use BERT tokenizer.\n",
        "# Why: BERT splits words into subwords. Example: \"playing\" -> \"play\", \"##ing\".\n",
        "# This creates a problem: labels are for words, but tokens become subwords.\n",
        "# So we must align labels to tokens (next step).\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Tokenize and align labels\n",
        "# =========================\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # Goal: Tokenize the input tokens (already split words).\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        is_split_into_words=True,  # Input is list of words (not a full string)\n",
        "        truncation=True,           # Cut if too long\n",
        "        padding=False              # No padding here (collator will do it later)\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "\n",
        "    # Goal: For each sentence in the batch, align labels to tokenized output.\n",
        "    for i in range(len(examples[\"tokens\"])):\n",
        "        # word_ids tells for each token which original word it came from.\n",
        "        # Example: [None, 0, 1, 1, 2, None]\n",
        "        # None = special token like [CLS] [SEP]\n",
        "        word_ids = tokenized.word_ids(batch_index=i)\n",
        "\n",
        "        prev_word_id = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                # Special tokens have no label.\n",
        "                # We use -100 so loss will ignore it.\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            elif word_id != prev_word_id:\n",
        "                # First sub-token of a word:\n",
        "                # take the real label for that word.\n",
        "                label_ids.append(examples[\"labels\"][i][word_id])\n",
        "\n",
        "            else:\n",
        "                # Other sub-tokens of the same word:\n",
        "                # ignore them in loss to avoid double-counting.\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            prev_word_id = word_id\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    # Goal: Put aligned labels inside tokenized output.\n",
        "    tokenized[\"labels\"] = aligned_labels\n",
        "    return tokenized\n",
        "\n",
        "data = data.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Output:\n",
        "# dataset now has fields like:\n",
        "# - input_ids\n",
        "# - attention_mask\n",
        "# - labels (aligned with input_ids length)"
      ],
      "metadata": {
        "id": "0NzNpX2Xu4Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load model for Token Classification (NER)**"
      ],
      "metadata": {
        "id": "J__ChADuwUTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model_name = 'bert-base-NER'  # Note: This variable is not used later.\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(all_labels),  # number of classes\n",
        "    id2label=id2label,           # id -> label name\n",
        "    label2id=label2id            # label name -> id\n",
        ")\n",
        "\n",
        "# Goal: Create a BERT model with a classification head for each token.\n",
        "# Why: NER predicts a label for every token (word/subword)."
      ],
      "metadata": {
        "id": "V0GwSQbEu8d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data collator for token classification**"
      ],
      "metadata": {
        "id": "YXG84cThwbYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Goal: Make batches with correct padding.\n",
        "# Why:\n",
        "# - Each sentence has different length.\n",
        "# - This collator pads input_ids and attention_mask.\n",
        "# - It also pads labels with -100 so loss ignores padding tokens."
      ],
      "metadata": {
        "id": "z8muLVNYvAoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "Ihw053_iwhzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training arguments\n",
        "# =========================\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./ner_bert_ar\",          # where to save outputs\n",
        "    learning_rate=2e-5,                 # small LR for BERT fine-tuning\n",
        "    per_device_train_batch_size=32,     # train batch size\n",
        "    num_train_epochs=1,                 # number of epochs\n",
        "    weight_decay=0.01,                  # regularization\n",
        "    logging_steps=10,                   # log every 10 steps\n",
        "    save_strategy=\"no\",                 # do not save checkpoints\n",
        "    report_to=\"none\",                   # no wandb / tensorboard reports\n",
        "\n",
        "    load_best_model_at_end=True,        # keep best model after training (needs evaluation + saving)\n",
        "    metric_for_best_model=\"f1\",         # best model = highest f1\n",
        "    per_device_eval_batch_size=32,      # eval batch size\n",
        "    # evaluation_strategy=\"epoch\"       # IMPORTANT: if commented, evaluation may NOT run automatically\n",
        ")\n",
        "\n",
        "# Important note:\n",
        "# - load_best_model_at_end=True usually needs:\n",
        "#   evaluation_strategy + save_strategy not \"no\"\n",
        "#   (otherwise Trainer may not know \"best model\" because it never saves checkpoints)"
      ],
      "metadata": {
        "id": "eCAT671TvFqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Metrics (F1, precision, recall...) using seqeval\n",
        "# =========================\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # p = (predictions, labels)\n",
        "    predictions, labels = p\n",
        "\n",
        "    # Goal: Convert model outputs (logits) to predicted class id.\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Goal: Remove tokens with label -100 (ignored tokens).\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        temp_preds = []\n",
        "        temp_labels = []\n",
        "\n",
        "        for p_i, l_i in zip(pred, lab):\n",
        "            if l_i != -100:\n",
        "                # Convert ids to label names (needed by seqeval)\n",
        "                temp_preds.append(id2label[p_i])\n",
        "                temp_labels.append(id2label[l_i])\n",
        "\n",
        "        true_predictions.append(temp_preds)\n",
        "        true_labels.append(temp_labels)\n",
        "\n",
        "    # Goal: Compute seqeval metrics for NER.\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    # Return main metrics to Trainer\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "LGfyggVbvKDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Trainer\n",
        "# =========================\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the NER model\n",
        "    args=args,                           # training settings\n",
        "    train_dataset=data[\"train\"],         # training split\n",
        "    eval_dataset=data[\"validation\"],     # validation split (important for F1)\n",
        "    # tokenizer=tokenizer,               # optional in new versions; can be used for saving / padding info\n",
        "    data_collator=data_collator,         # padding + label padding\n",
        "    compute_metrics=compute_metrics       # calculate F1 etc.\n",
        ")\n",
        "\n",
        "# Goal: Trainer controls training loop:\n",
        "# - batching\n",
        "# - forward/backward\n",
        "# - evaluation\n",
        "# - logging"
      ],
      "metadata": {
        "id": "yO2NRFA_ue64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Train the model\n",
        "# =========================\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Goal: Fine-tune BERT on your NER dataset.\n",
        "# Output: Trained model weights (in memory, and maybe in output_dir depending on settings)"
      ],
      "metadata": {
        "id": "2xrHHUZLvPLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Train the model\n",
        "# =========================\n",
        "\n",
        "test_results = trainer.evaluate(data[\"test\"])\n",
        "\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "PTym65NgswBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Save the model\n",
        "# =========================\n",
        "\n",
        "# Define folder name\n",
        "save_path = \"./my_ner_model\"\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(save_path)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# A folder will be created:\n",
        "'''\n",
        "my_ner_model/\n",
        "    config.json\n",
        "    pytorch_model.bin\n",
        "    tokenizer.json\n",
        "    vocab.txt\n",
        "    special_tokens_map.json\n",
        "    tokenizer_config.json\n",
        "'''"
      ],
      "metadata": {
        "id": "IKs4tk9EsxA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "d_K59pCrwnr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Goal: Create NER pipeline (token-classification)\n",
        "ner_pipe = pipeline(\n",
        "    task=\"token-classification\",\n",
        "    model=\"./my_ner_model\",          # folder path (or use your model object)\n",
        "    tokenizer=\"./my_ner_model\",\n",
        "    aggregation_strategy=\"simple\"    # merge sub-tokens into full words\n",
        ")"
      ],
      "metadata": {
        "id": "hGeYHJMJt5A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "\n",
        "text = \"Ahmed is skilled in Python, Java, and Machine Learning.\"\n",
        "preds = ner_pipe(text)\n",
        "\n",
        "#  Print results\n",
        "for p in preds:\n",
        "    print(p)\n",
        "\n",
        "# O,O,O,O, B-SKILL , O ,  B-SKILL , O , O , B-SKILL , O , I-SKILL , O"
      ],
      "metadata": {
        "id": "VivzxO0Nvosl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**âœ… Project Summary**\n",
        "\n",
        "In this project, we trained a **BERT model for Named Entity Recognition (NER)**.\n",
        "\n",
        "We loaded a prepared dataset that contains:\n",
        "\n",
        "* Tokens (words)\n",
        "* NER labels for each word\n",
        "\n",
        "Then we:\n",
        "\n",
        "* Converted labels to numeric IDs\n",
        "* Tokenized the text using BERT tokenizer\n",
        "* Aligned labels with subword tokens\n",
        "* Built a token classification model using `bert-base-uncased`\n",
        "* Used HuggingFace `Trainer` for training\n",
        "* Evaluated the model using Precision, Recall, F1-score, and Accuracy\n",
        "\n",
        "After training, the model can:\n",
        "\n",
        "* Predict entity labels for each word in a sentence\n",
        "* Be saved and used in a backend system\n",
        "* Be used with pipeline for inference\n",
        "\n",
        "This notebook shows the full workflow:\n",
        "Dataset â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Ready Model ðŸš€\n"
      ],
      "metadata": {
        "id": "KnJeMuPvzcJW"
      }
    }
  ]
}